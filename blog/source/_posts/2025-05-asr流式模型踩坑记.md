---
title: asr流式模型踩坑记（一）
tags:
  - python
  - asr
  - stream
  - ai model
category:
  - ai
mermaid: false
date: 2025-05-31 22:44:25
---
# 流式ASR实现踩坑记录：从ModelScope到FunASR ONNX的技术探索

## 前言

在开发实时语音识别系统时，流式ASR（Automatic Speech Recognition）是一个关键技术。本文记录了我在实现流式ASR过程中遇到的各种技术难点和解决方案，主要涉及ModelScope Pipeline和FunASR ONNX两种实现方式的对比与优化。

## 技术背景

### 使用的模型
- **模型名称**: `iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online`
- **模型版本**: v2.0.4
- **支持格式**: 16kHz采样率，中文语音识别
- **特点**: 支持在线流式识别，适合实时应用场景

## 实现方案对比

### 方案一：ModelScope Pipeline实现

```python
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks

# 初始化pipeline
inference_pipeline = pipeline(
    task=Tasks.auto_speech_recognition,
    model='iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online',
    model_revision='v2.0.4',
)

# 流式处理
chunk_size = [8, 8, 4]  # 480ms chunks
encoder_chunk_look_back = 4
decoder_chunk_look_back = 1
stride_size = chunk_size[1] * 960

cache = {}
for sample_offset in range(0, speech_length, min(stride_size, speech_length - sample_offset)):
    if sample_offset + stride_size >= speech_length - 1:
        stride_size = speech_length - sample_offset
        is_final = True
    
    res = inference_pipeline(
        speech[sample_offset: sample_offset + stride_size], 
        cache=cache, 
        is_final=is_final, 
        encoder_chunk_look_back=encoder_chunk_look_back, 
        decoder_chunk_look_back=decoder_chunk_look_back
    )
```

### 方案二：FunASR ONNX实现

```python
from funasr_onnx.paraformer_online_bin import Paraformer
from modelscope import snapshot_download

# 下载并初始化ONNX模型
model_dir = snapshot_download('iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx')
model = Paraformer(
    model_dir, 
    batch_size=1, 
    quantize=True, 
    chunk_size=[8, 8, 4], 
    intra_op_num_threads=4
)

# 流式处理
step = chunk_size[1] * 960
param_dict = {'cache': dict()}
final_result = ""

for sample_offset in range(0, speech_length, min(step, speech_length - sample_offset)):
    if sample_offset + step >= speech_length - 1:
        step = speech_length - sample_offset
        is_final = True
    else:
        is_final = False
    
    param_dict['is_final'] = is_final
    rec_result = model(
        audio_in=speech[sample_offset: sample_offset + step],
        param_dict=param_dict
    )
    
    if len(rec_result) > 0:
        final_result += rec_result[0]["preds"][0]
```

## 主要踩坑点及解决方案

### 1. ModelScope的理想与现实差距

**问题**: 最初计划只使用ModelScope来统一管理各种模型

**踩坑经历**:
- 认为ModelScope可以方便地使用各种模型，想要统一技术栈
- 实际测试发现ModelScope对流式模型的支持并不完善，比如 ONNX 模型就不太支持


**经验总结**:
- ModelScope目前对流式ASR的支持还不够成熟
- 真正的流式ASR需要使用专门的流式引擎（如FunASR ONNX）
- 不要被框架的"万能"宣传迷惑，要根据具体技术需求选择合适的工具
- 流式处理和批处理是完全不同的技术路径，需要专门的优化

### 2. 音频分块参数调优

**问题**: chunk_size参数对识别效果和延迟影响巨大

**踩坑经历**:
- 初始使用 `[5, 10, 5]` (600ms)，延迟较高
- 调整为 `[8, 8, 4]` (480ms)，在延迟和准确性间找到平衡

**解决方案**:
```python
# 推荐配置
chunk_size = [8, 8, 4]  # [encoder_chunk, decoder_chunk, lookahead]
stride_size = chunk_size[1] * 960  # 计算步长
```

**经验总结**:
- `encoder_chunk`: 影响编码器处理的音频长度
- `decoder_chunk`: 影响解码器输出的延迟
- `lookahead`: 前瞻帧数，影响识别准确性

### 3. 缓存机制的正确使用

**问题**: 缓存状态管理不当导致识别结果错乱

**踩坑经历**:
- 忘记在不同会话间清理缓存
- 缓存对象在多线程环境下出现竞争条件

**解决方案**:
```python
# ModelScope方式
cache = {}  # 每个会话独立的缓存

# FunASR ONNX方式
param_dict = {'cache': dict()}  # 确保缓存隔离
```

### 4. 最终帧标记的重要性

**问题**: `is_final`标记使用不当影响识别完整性

**踩坑经历**:
- 未正确设置最后一帧的`is_final=True`
- 导致最后一段音频识别不完整

**解决方案**:
```python
# 正确的边界处理
if sample_offset + stride_size >= speech_length - 1:
    stride_size = speech_length - sample_offset
    is_final = True
```

### 5. 性能优化对比

| 特性 | ModelScope Pipeline | FunASR ONNX |
|------|---------------------|-------------|
| **初始化速度** | 较慢                  | 快 |
| **部署复杂度** | 简单                  | 中等 |
| **量化支持** | 不支持                 | 完整支持 |

### 6. 环境配置踩坑

**问题**: 模型缓存和依赖管理

**踩坑经历**:
- ModelScope默认缓存路径权限问题
- ONNX运行时版本兼容性问题

### 7. 多线程并发处理

**问题**: ONNX模型的线程安全性

**解决方案**:
```python
# 控制ONNX推理线程数
model = Paraformer(
    model_dir, 
    batch_size=1,  # 目前只支持batch_size=1
    quantize=True,
    intra_op_num_threads=4  # 根据CPU核心数调整
)
```

## 性能测试结果 （mac M3 Pro）

### 延迟对比
- **ModelScope Pipeline**: 800ms+
- **FunASR ONNX**: ~500-600ms

## 最佳实践建议

### 1. 选择合适的实现方案
- **开发阶段**: 使用ModelScope Pipeline，调试方便
- **生产环境**: 使用FunASR ONNX，性能更优

### 2. 参数调优策略
```python
# 低延迟场景
chunk_size = [6, 6, 4]  # 更小的chunk

# 高准确性场景
chunk_size = [10, 10, 6]  # 更大的chunk和lookahead
```

### 3. 错误处理
```python
try:
    res = inference_pipeline(...)
    if len(res) > 0 and "value" in res[0]:
        # 处理识别结果
        pass
except Exception as e:
    logger.error(f"ASR processing failed: {e}")
    # 错误恢复逻辑
```

### 4. 内存管理
```python
# 定期清理缓存
if len(cache) > MAX_CACHE_SIZE:
    cache.clear()
```

## 总结

流式ASR的实现涉及多个技术细节，从模型选择、参数调优到性能优化，每个环节都可能成为瓶颈。通过对比ModelScope Pipeline和FunASR ONNX两种方案，我们发现：

1. **FunASR ONNX在生产环境中表现更优**，特别是在延迟和资源消耗方面
2. **参数调优是关键**，需要根据具体应用场景平衡延迟和准确性
3. **缓存管理和状态控制**是流式处理的核心难点
4. **环境配置和依赖管理**需要特别注意版本兼容性

希望这些踩坑经验能帮助其他开发者更快地实现稳定可靠的流式ASR系统。

## 相关资源

- [ModelScope官方文档](https://modelscope.cn/)
- [FunASR项目地址](https://github.com/alibaba-damo-academy/FunASR)
- [Paraformer模型详情](https://modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online)

---

*本文基于实际开发经验总结，如有问题欢迎交流讨论。*